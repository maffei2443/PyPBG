{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "import gc  # Consumo de memória é muito alto, então chamada\n",
    "            # ao coletor de lixo ajuda um pouco\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import importlib\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import util\n",
    "import upbg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = mlflow.tracking.MlflowClient()\n",
    "# run = client.create_run(\"0\")\n",
    "# print(\"RUN_ID:\", mlflow.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGS:  Namespace(alpha=0.005, beta=0.001, data_size='small', global_max_itr=1, global_threshold=1, lines_percentage=1, local_max_itr=1, local_threshold=1, n_components=100, ngram_max=1, ngram_min=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Run PBG on jurix2020 corpus.')\n",
    "parser.add_argument('--n_components', type=int, default=100)\n",
    "parser.add_argument('--alpha', type=float, default=0.005)\n",
    "parser.add_argument('--beta', type=float, default=0.001)\n",
    "parser.add_argument('--local_max_itr', type=int, default=1)\n",
    "parser.add_argument('--global_max_itr', type=int, default=1)\n",
    "parser.add_argument('--local_threshold', type=float, default=1)\n",
    "parser.add_argument('--global_threshold', type=float, default=1)\n",
    "parser.add_argument('--ngram_min', type=int, default=1)\n",
    "parser.add_argument('--ngram_max', type=int, default=1)\n",
    "parser.add_argument('--lines_percentage', type=float, default=1, \n",
    "    help='How many lines will be used for train/test/validation datasets',\n",
    ")\n",
    "# parser.add_argument('--huge_mem', type=int, default=0,)\n",
    "# parser.add_argument('--use_spacy', type=int, default=0,)\n",
    "parser.add_argument('--data_size', type=str, default='small',\n",
    "    choices=['small', 'medium'],\n",
    "    help='Which dataset will be used for train/test/validation ',\n",
    ")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "dict_args = args.__dict__\n",
    "print(\"ARGS: \", args)\n",
    "open('log.txt', 'w').write(str(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE=dict_args.pop('data_size')\n",
    "VALIDATION_DATA_PATH='csv/validation_{}.csv'.format(DATA_SIZE)\n",
    "TRAIN_DATA_PATH='csv/train_{}.csv'.format(DATA_SIZE)\n",
    "TEST_DATA_PATH='csv/test_{}.csv'.format(DATA_SIZE)\n",
    "\n",
    "THEMES = [\n",
    "    5, 6, 26, 33, 139, 163, 232, 313, 339, 350, 406, 409,\n",
    "    555, 589, 597, 634, 660, 695, 729, 766, 773, 793, 800,\n",
    "    810, 852, 895, 951, 975\n",
    "]\n",
    "\n",
    "\n",
    "def groupby_process(df):\n",
    "    new_df = df.sort_values(['process_id', 'page'])\n",
    "    new_df = new_df.groupby(\n",
    "                ['process_id', 'themes'],\n",
    "                group_keys=False\n",
    "            ).apply(lambda x: x.body.str.cat(sep=' ')).reset_index()\n",
    "    new_df = new_df.rename(index=str, columns={0: \"body\"})\n",
    "    return new_df\n",
    "\n",
    "# Nota: para rápida iteração, limitar qtd de linhas carregadas\n",
    "def get_data(path, preds=None, key=None, lines_per=.02):\n",
    "    data = pd.read_csv(path)\n",
    "    if lines_per is not None:\n",
    "        lines = int(lines_per * data.shape[0])\n",
    "        data = data.iloc[:lines, :]\n",
    "    \n",
    "    data = data.rename(columns={ 'pages': 'page'})\n",
    "#     data[\"preds\"] = preds[key]\n",
    "#     data = data[data[\"preds\"] != \"outros\"]\n",
    "    data = groupby_process(data)\n",
    "    \n",
    "#     data.themes = data.themes.apply(lambda x: literal_eval(x))\n",
    "    data.themes = data.themes.apply(lambda x: eval(x))\n",
    "    return data\n",
    "\n",
    "def transform_y(train_labels, test_labels):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit(train_labels)\n",
    "\n",
    "    mlb_train = mlb.transform(train_labels)\n",
    "    mlb_test = mlb.transform(test_labels)\n",
    "\n",
    "    print(mlb.classes_)\n",
    "\n",
    "    return mlb_train, mlb_test, mlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_TRAIN_TYPE: <class 'str'>\n",
      "MAX_TRAIN_LEN: 7527991\n",
      "MAX_TEST_LEN: 1948317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = get_data(TRAIN_DATA_PATH, lines_per=dict_args['lines_percentage'])\n",
    "test_data = get_data(TEST_DATA_PATH, lines_per=dict_args['lines_percentage'])\n",
    "\n",
    "print(\"MAX_TRAIN_TYPE:\", type(train_data.body[0]))\n",
    "print(\"MAX_TRAIN_LEN:\", max(map(len, train_data.body)))\n",
    "print(\"MAX_TEST_LEN:\", max(map(len, test_data.body)))\n",
    "\n",
    "dict_args.pop('lines_percentage', None)\n",
    "# validation_data = get_data(VALIDATION_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   5   6  26  33 139 163 232 313 339 350 406 409 555 589 597 634 660\n",
      " 695 729 766 773 793 800 810 852 895 951 975]\n",
      "X_train: (2743,), \n",
      "\ty_train: (2743, 29)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.themes = train_data.themes.apply(\n",
    "    lambda x: list(set(sorted([i if i in THEMES else 0 for i in x])))\n",
    ")\n",
    "test_data.themes = test_data.themes.apply(\n",
    "    lambda x: list(set(sorted([i if i in THEMES else 0 for i in x])))\n",
    ")\n",
    "# validation_data.themes = validation_data.themes.apply(lambda x: list(set(sorted([i if i in THEMES else 0 for i in x]))))\n",
    "\n",
    "\n",
    "y_train, y_test, mlb = transform_y(train_data.themes, test_data.themes)\n",
    "\n",
    "X_train = train_data.body\n",
    "X_test = test_data.body\n",
    "X_train_themes = train_data.themes\n",
    "\n",
    "print('X_train: {}, \\n\\ty_train: {}'.format(X_train.shape, y_train.shape))\n",
    "# print('X_test: {}, \\n\\ty_test: {}'.format(X_test.shape, y_test.shape))\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(dict_args.pop('ngram_min'), dict_args.pop('ngram_max')),\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "# X_valid = vectorizer.transform(validation_data.body)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "# y_valid = mlb.transform(validation_data.themes)\n",
    "\n",
    "del X_train, X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import util\n",
    "importlib.reload(util)\n",
    "# DISCLAIMER: só pode ser executada uma vez (não pergunte o motivo,\n",
    "# mas parece ter a ver com `docs` do SimplePreprocessingBR)\n",
    "\n",
    "print('preprocessing...')\n",
    "params=dict(\n",
    "    use_nltk=True,\n",
    "    extra_stop_words=[i.lower().strip() for i in open('stopwords.txt').readlines()],\n",
    ")\n",
    "\n",
    "pp = util.SimplePreprocessingBR_Lite(**params)\n",
    "M_train = pp.transform(train_data.body)\n",
    "M_test = pp.transform(test_data.body)\n",
    "\n",
    "\n",
    "print('done.')\n",
    "\n",
    "categories = set((itertools.chain(*train_data.themes)))\n",
    "n_class = len(categories)\n",
    "print(f'nclass {n_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pp, test_data, train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M_train_vectorized = vectorizer.fit_transform(M_train)\n",
    "del M_train; gc.collect()\n",
    "\n",
    "M_test_vectorized = vectorizer.transform(M_test)\n",
    "del M_test; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(upbg)\n",
    "hyperparams=dict_args\n",
    "\n",
    "pbg = upbg.UPBG(\n",
    "    **hyperparams,\n",
    "    feature_names=vectorizer.get_feature_names(),\n",
    ")\n",
    "dict_args.clear()\n",
    "del hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fitting...\")\n",
    "# pbg.fit(M_train, newsgroups_train.target)\n",
    "pbg.fit(\n",
    "    M_train_vectorized,\n",
    "    X_train_themes,\n",
    ")\n",
    "print('done')\n",
    "\n",
    "mlflow.sklearn.log_model(pbg, 'pbg_model_spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vai fazer dump tema-topico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json \n",
    "\n",
    "temas = json.load(open('temas.json')) \n",
    "tema_topico = {} \n",
    "topicos = pbg.get_topics(20) \n",
    "for tema, topico in pbg.map_class_.items(): \n",
    "    if tema > 0: \n",
    "        tema = str(tema)         \n",
    "        tema_topico[temas[tema]] = topicos[topico] \n",
    "mlflow.log_dict(tema_topico, \"tema_topico.json\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
