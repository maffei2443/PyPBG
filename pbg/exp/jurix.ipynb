{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import importlib\n",
    "import itertools\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from util import SimplePreprocessingBR\n",
    "import upbg\n",
    "\n",
    "VALIDATION_DATA_PATH='csv/validation_small.csv'\n",
    "TRAIN_DATA_PATH='csv/train_small.csv'\n",
    "TEST_DATA_PATH='csv/test_small.csv'\n",
    "\n",
    "THEMES = [\n",
    "    5, 6, 26, 33, 139, 163, 232, 313, 339, 350, 406, 409,\n",
    "    555, 589, 597, 634, 660, 695, 729, 766, 773, 793, 800,\n",
    "    810, 852, 895, 951, 975\n",
    "]\n",
    "LINES_PERCENTAGE = .05\n",
    "\n",
    "\n",
    "\n",
    "def groupby_process(df):\n",
    "    new_df = df.sort_values(['process_id', 'page'])\n",
    "    new_df = new_df.groupby(\n",
    "                ['process_id', 'themes'],\n",
    "                group_keys=False\n",
    "            ).apply(lambda x: x.body.str.cat(sep=' ')).reset_index()\n",
    "    new_df = new_df.rename(index=str, columns={0: \"body\"})\n",
    "    return new_df\n",
    "\n",
    "# Nota: para rápida iteração, limitar qtd de linhas carregadas\n",
    "def get_data(path, preds=None, key=None, lines_per=.02):\n",
    "    data = pd.read_csv(path)\n",
    "    if lines_per is not None:\n",
    "        lines = int(lines_per * data.shape[0])\n",
    "        data = data.iloc[:lines, :]\n",
    "    \n",
    "    data = data.rename(columns={ 'pages': 'page'})\n",
    "#     data[\"preds\"] = preds[key]\n",
    "#     data = data[data[\"preds\"] != \"outros\"]\n",
    "    data = groupby_process(data)\n",
    "    \n",
    "#     data.themes = data.themes.apply(lambda x: literal_eval(x))\n",
    "    data.themes = data.themes.apply(lambda x: eval(x))\n",
    "    return data\n",
    "\n",
    "def transform_y(train_labels, test_labels):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit(train_labels)\n",
    "\n",
    "    mlb_train = mlb.transform(train_labels)\n",
    "    mlb_test = mlb.transform(test_labels)\n",
    "\n",
    "    print(mlb.classes_)\n",
    "\n",
    "    return mlb_train, mlb_test, mlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Run PBG on jurix2020 corpus.')\n",
    "parser.add_argument('--n_components', type=int, default=100)\n",
    "parser.add_argument('--alpha', type=float, default=0.005)\n",
    "parser.add_argument('--beta', type=float, default=0.001)\n",
    "parser.add_argument('--local_max_itr', type=int, default=1)\n",
    "parser.add_argument('--global_max_itr', type=int, default=1)\n",
    "parser.add_argument('--local_threshold', type=float, default=1)\n",
    "parser.add_argument('--global_threshold', type=float, default=1)\n",
    "parser.add_argument('--ngram_min', type=int, default=1)\n",
    "parser.add_argument('--ngram_max', type=int, default=1)\n",
    "parser.add_argument('--lines_percentage', type=float, default=.1, \n",
    "    help='How many lines will be used for train/test/validation datasets',\n",
    ")\n",
    "parser.add_argument('--huge_mem', type=int, default=0,)\n",
    "parser.add_argument('--use_spacy', type=int, default=0,)\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "dict_args = args.__dict__\n",
    "print(\"ARGS: \", args)\n",
    "open('log.txt', 'w').write(str(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.41 s, sys: 198 ms, total: 2.61 s\n",
      "Wall time: 2.61 s\n"
     ]
    }
   ],
   "source": [
    "train_data = get_data(TRAIN_DATA_PATH, lines_per=dict_args['lines_percentage'])\n",
    "test_data = get_data(TEST_DATA_PATH, lines_per=dict_args['lines_percentage'])\n",
    "\n",
    "dict_args.pop('lines_percentage', None)\n",
    "# validation_data = get_data(VALIDATION_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   5   6  26  33 139 163 232 313 339 350 406 409 555 589 634 660 729\n",
      " 766 773 793 800 810 852 895 975]\n",
      "X_train: (286,), \n",
      "\ty_train: (286, 26)\n",
      "CPU times: user 3.06 s, sys: 50.9 ms, total: 3.11 s\n",
      "Wall time: 3.1 s\n"
     ]
    }
   ],
   "source": [
    "train_data.themes = train_data.themes.apply(\n",
    "    lambda x: list(set(sorted([i if i in THEMES else 0 for i in x])))\n",
    ")\n",
    "test_data.themes = test_data.themes.apply(\n",
    "    lambda x: list(set(sorted([i if i in THEMES else 0 for i in x])))\n",
    ")\n",
    "# validation_data.themes = validation_data.themes.apply(lambda x: list(set(sorted([i if i in THEMES else 0 for i in x]))))\n",
    "\n",
    "\n",
    "y_train, y_test, mlb = transform_y(train_data.themes, test_data.themes)\n",
    "\n",
    "X_train = train_data.body\n",
    "X_test = test_data.body\n",
    "print('X_train: {}, \\n\\ty_train: {}'.format(X_train.shape, y_train.shape))\n",
    "# print('X_test: {}, \\n\\ty_test: {}'.format(X_test.shape, y_test.shape))\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(dict_args.pop('ngram_min'), dict_args.pop('ngram_max')),\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "# X_valid = vectorizer.transform(validation_data.body)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "# y_valid = mlb.transform(validation_data.themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "done.\n",
      "CPU times: user 6min 17s, sys: 10.9 s, total: 6min 28s\n",
      "Wall time: 6min 28s\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "importlib.reload(util)\n",
    "# DISCLAIMER: só pode ser executada uma vez (não pergunte o motivo,\n",
    "# mas parece ter a ver com `docs` do SimplePreprocessingBR)\n",
    "\n",
    "print('preprocessing...')\n",
    "params=dict(\n",
    "    use_nltk=True,\n",
    "    extra_stop_words=[i.lower().strip() for i in open('stopwords.txt').readlines()],\n",
    "    huge_mem=dict_args.pop('huge_mem'),\n",
    "    use_spacy=dict_args.pop('use_spacy'),\n",
    ")\n",
    "\n",
    "print(\"MAX_TRAIN___:\", type(train_data.body[0]))\n",
    "print(\"MAX_TRAIN:\", max(map(len, train_data.body)))\n",
    "print(\"MAX_TEST:\", max(map(len, test_data.body)))\n",
    "\n",
    "pp = util.SimplePreprocessingBR(**params)\n",
    "\n",
    "\n",
    "M_train = pp.transform(train_data.body)\n",
    "M_test = pp.transform(test_data.body)\n",
    "\n",
    "print('done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del pp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nclass 26\n"
     ]
    }
   ],
   "source": [
    "M_train_vectorized = vectorizer.fit_transform(M_train)\n",
    "M_test_vectorized = vectorizer.transform(M_test)\n",
    "\n",
    "# train_data.to_csv(\"csv/train_ready.csv\")\n",
    "# test_data.to_csv(\"csv/test_ready.csv\")\n",
    "# validation_data.to_csv(\"csv/validation_ready.csv\")\n",
    "\n",
    "categories = set((itertools.chain(*train_data.themes)))\n",
    "n_class = len(categories)\n",
    "\n",
    "print(f'nclass {n_class}')\n",
    "\n",
    "labels_raw = [tuple(i) for i in train_data.themes]\n",
    "mapa = dict([i[::-1] for i in enumerate(set(itertools.chain(*labels_raw)))])\n",
    "labels_mp = [tuple([mapa[jj] for jj in i]) for i in labels_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(upbg)\n",
    "\n",
    "# hyperparams = dict(\n",
    "#     n_components=K,\n",
    "#     alpha=0.005,\n",
    "#     beta=0.001,\n",
    "#     local_max_itr=50,\n",
    "#     global_max_itr=15,\n",
    "#     local_threshold=1e-6,\n",
    "#     global_threshold=1e-6,\n",
    "# )\n",
    "\n",
    "hyperparams=dict_args\n",
    "\n",
    "pbg = upbg.UPBG(\n",
    "    **hyperparams,\n",
    "    feature_names=vectorizer.get_feature_names(),\n",
    ")\n",
    "dict_args.clear()\n",
    "del hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "docs processed (itr 0):   0%|          | 0/285 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "docs processed (itr 0): 100%|##########| 285/285 [00:17<00:00, 16.64it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:20<00:00, 1613.97it/s]\n",
      "docs processed (itr 1): 100%|##########| 285/285 [00:18<00:00, 15.31it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:19<00:00, 1620.09it/s]\n",
      "docs processed (itr 2): 100%|##########| 285/285 [00:17<00:00, 15.98it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:20<00:00, 1588.52it/s]\n",
      "docs processed (itr 3): 100%|##########| 285/285 [00:18<00:00, 15.50it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:20<00:00, 1609.72it/s]\n",
      "docs processed (itr 4): 100%|##########| 285/285 [00:17<00:00, 16.35it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:19<00:00, 1640.93it/s]\n",
      "docs processed (itr 5): 100%|##########| 285/285 [00:15<00:00, 18.10it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:20<00:00, 1591.36it/s]\n",
      "docs processed (itr 6): 100%|##########| 285/285 [00:15<00:00, 18.58it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:20<00:00, 1612.49it/s]\n",
      "docs processed (itr 7): 100%|##########| 285/285 [00:14<00:00, 19.11it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:20<00:00, 1592.91it/s]\n",
      "docs processed (itr 8): 100%|##########| 285/285 [00:14<00:00, 19.43it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:19<00:00, 1644.83it/s]\n",
      "docs processed (itr 9): 100%|##########| 285/285 [00:14<00:00, 19.76it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:20<00:00, 1598.03it/s]\n",
      "docs processed (itr 10): 100%|##########| 285/285 [00:14<00:00, 19.64it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:20<00:00, 1602.96it/s]\n",
      "docs processed (itr 11): 100%|##########| 285/285 [00:14<00:00, 19.44it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:20<00:00, 1606.12it/s]\n",
      "docs processed (itr 12): 100%|##########| 285/285 [00:14<00:00, 19.27it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:19<00:00, 1628.80it/s]\n",
      "docs processed (itr 13): 100%|##########| 285/285 [00:14<00:00, 19.20it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:20<00:00, 1607.20it/s]\n",
      "docs processed (itr 14): 100%|##########| 285/285 [00:14<00:00, 19.61it/s]\n",
      "global propagation:   : 100%|##########| 32334/32334 [00:20<00:00, 1600.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"fitting...\")\n",
    "# pbg.fit(M_train, newsgroups_train.target)\n",
    "pbg.fit(\n",
    "    M_train_vectorized,\n",
    "    train_data.themes,\n",
    ")\n",
    "print('done')\n",
    "\n",
    "mlflow.sklearn.log_model(pbg, 'pbg_model_spacy')"
   ]
  },
  {
   "source": [
    "## Vai fazer dump tema-topico"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json \n",
    "\n",
    "temas = json.load(open('temas.json')) \n",
    "tema_topico = {} \n",
    "topicos = pbg.get_topics(20) \n",
    "for tema, topico in pbg.map_class_.items(): \n",
    "    if tema > 0: \n",
    "        tema = str(tema)         \n",
    "        tema_topico[temas[tema]] = topicos[topico] \n",
    "mlflow.log_dict(tema_topico, \"tema_topico.json\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}